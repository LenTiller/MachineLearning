{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 1 - Graded Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers shown are: \n",
      " [[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "#Create two rows with numbers\n",
    "firstrow = np.hstack(digits.images[:5,:,:])\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "\n",
    "plt.imshow(np.vstack((firstrow,secondrow)))\n",
    "print (\"The numbers shown are: \\n\",np.vstack((digits.target[:5], \n",
    "                                             digits.target[5:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make a prediction function h\n",
    "def prediction_function(x,theta):\n",
    "    x_predict = 1/(1+np.e**-(x @ theta))\n",
    "    return x_predict\n",
    "\n",
    "#Use the output of that function to compute the cost function J:\n",
    "def cost_function(x_predict,y,theta):\n",
    "    cost = ((-y).T @ np.log(x_predict)-(1-y).T @ np.log(1-x_predict))/len(y)\n",
    "    return cost\n",
    "\n",
    "#Create a function that returns the gradient values, given h (x_predict), y and x:\n",
    "def compute_gradient(x_predict, y, x):\n",
    "    dtheta = (x_predict.T-y) @ x / len(y)\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 25\n",
    "\n",
    "x = np.reshape(digits.images[:1500],(1500,64))\n",
    "x_test = np.reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "target = digits.target[:1500]\n",
    "target_test = digits.target[1500:]\n",
    "\n",
    "\n",
    "# calculate theta-vectors for each one-vs-all case; save in one 2D-array theta\n",
    "def minimisation(alpha,theta):\n",
    "    for i in range(iterations):\n",
    "        for j in range(theta.shape[1]):\n",
    "        #calculate a prediction for one-vs-all case j\n",
    "            x_predict = prediction_function(x,theta[:,j])\n",
    "        \n",
    "        #set all values in target vecot not equal to j to 0\n",
    "            y = np.zeros(x.shape[0])\n",
    "            y[target == j] = 1\n",
    "         \n",
    "        #calculate theta for one-vs-all case j\n",
    "            theta[:,j] = theta[:,j] - alpha * compute_gradient(x_predict, y, x)\n",
    "    return theta\n",
    "    \n",
    "# calculate the score of the final prediction function\n",
    "\n",
    "def test_prediction(x_test, target_test, theta):\n",
    "    \n",
    "    # calculate array with all predicted values for target\n",
    "    predictions = prediction_function(x_test,theta).argmax(axis=1)\n",
    "\n",
    "    #calculate prediction score as fraction of correct predictions and \n",
    "    #total number of instances in target_test\n",
    "    prediction_score = sum(target_test == predictions) / len(target_test)\n",
    "    return prediction_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of my prediction is: 0.855218855219\n"
     ]
    }
   ],
   "source": [
    "# calculates scores for range a of alphas\n",
    "def test_alphas(alpha_vector):\n",
    "    score_vector = np.array([])\n",
    "    for i in range(len(alpha_vector)):\n",
    "        theta = minimisation(alpha_vector[i], np.zeros((64,10)))\n",
    "        score_vector = np.append(score_vector,test_prediction(x_test,target_test,theta))\n",
    "        theta = np.zeros((64,10))\n",
    "    return score_vector\n",
    "\n",
    "#finds the best alpha based on calculated scores\n",
    "def findBestAlpha(score_vector, alpha_vector):\n",
    "    index = score_vector.argmax()\n",
    "    return alpha_vector[index]\n",
    "\n",
    "alpha_vector = np.linspace(0.001,0.1,100)\n",
    "\n",
    "score_vector = test_alphas(alpha_vector)\n",
    "\n",
    "# stores best alpha value for further use\n",
    "bestAlpha = findBestAlpha(score_vector, alpha_vector)\n",
    "\n",
    "theta_opt = minimisation(0.01, np.zeros((64,10)))\n",
    "\n",
    "print(\"The score of my prediction is:\", test_prediction(x_test,target_test,theta_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSummary:\\nThe first three functions are very much alike the three functions from the\\nlinear-regression assignment (vectorised solution). The minimisation takes \\ntwo nested loops, one for the iteratinos, one to apply gradient descent to each of the\\nn (number of classes) prediction-functions. The score of the function is a little \\ninaccurate. In fact, the value of a two-class [0,1] prediction function has a statistical \\nrelevance (P(Y=1; x theta)). In our case, this information gets lost, since we only use\\nthe index of the max of the prediction_function output to predict a class. This loss of\\ninformation is mirrored in the calculated score, that does not take into account\\nwith what confidence the respective class is predicted. \\n\\nIt would be possible to determin a stop condition for the iterations. However, \\nsince we calculate for n different prediction_functions, this stop condition \\nwould have to apply for every hypothesis-function individually. Such a function\\ncauses unnecessary convolution. It is thus more reasonable to choose a higher value\\nfor iterations. This holds particularly, since we choose an optimal value for \\nalpha. \\n\\nGiven a prediction score of 85% as a target value, my own prediction score of\\n0.8181, calculated from the testing_sample, seems to suffice to assume prevention\\nof overfitting\\n\\n    Finally, the calculation of an optimised alpha is very time intensive \\nand not efficient. It only serves the purpose \\nto show that a perfect alpha can be gained by comparison to other alphas. \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Summary:\n",
    "The first three functions are very much alike the three functions from the\n",
    "linear-regression assignment (vectorised solution). The minimisation takes \n",
    "two nested loops, one for the iteratinos, one to apply gradient descent to each of the\n",
    "n (number of classes) prediction-functions. The score of the function is a little \n",
    "inaccurate. In fact, the value of a two-class [0,1] prediction function has a statistical \n",
    "relevance (P(Y=1; x theta)). In our case, this information gets lost, since we only use\n",
    "the index of the max of the prediction_function output to predict a class. This loss of\n",
    "information is mirrored in the calculated score, that does not take into account\n",
    "with what confidence the respective class is predicted. \n",
    "\n",
    "It would be possible to determin a stop condition for the iterations. However, \n",
    "since we calculate for n different prediction_functions, this stop condition \n",
    "would have to apply for every hypothesis-function individually. Such a function\n",
    "causes unnecessary convolution. It is thus more reasonable to choose a higher value\n",
    "for iterations. This holds particularly, since we choose an optimal value for \n",
    "alpha. \n",
    "\n",
    "Given a prediction score of 85% as a target value, my own prediction score of\n",
    "0.8181, calculated from the testing_sample, seems to suffice to assume prevention\n",
    "of overfitting\n",
    "\n",
    "    Finally, the calculation of an optimised alpha is very time intensive \n",
    "and not efficient. It only serves the purpose \n",
    "to show that a perfect alpha can be gained by comparison to other alphas. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Graded Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![exercise 1](Assignment1_Part2_1.jpg)\n",
    "![exercise 3](Assignment1_Part2_2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
